"""
Contains all the demo_code used in the first tab of PyClust Demo, used for loading or generating data.
"""
import os
import json
import sys
import traceback

import pandas as pd
from gensim.utils import pickle
from sklearn.preprocessing import MinMaxScaler
from sklearn.datasets import make_blobs, make_moons
import gradio as gr

import sys
from pyclustkit.metalearning import MFExtractor
import pyclustkit
import pickle

# ----------------------Load Data----------------------------------------------------
def change_data_update_visibility(operations_status):
    """
    Controls visibility when the user wants to change the current working dataset.
    Returns:
        tuple: A tuple that contains visibility and content updates for Gradio components.
            (1) Data method Row - Visibility
            (2) Data Success Row - Visibility
            (3) Data Method Choice - Value

    """
    operations_status["meta-features-extraction"] = False
    operations_status["configuration-search"] = False
    print(operations_status)
    return gr.update(visible=True), gr.update(visible=False), gr.update(value=None), operations_status

def load_csv(file, csv_options, data_id):
    """
    Loads a CSV dataset and updates UI.
    Args:
        file (file-like object or None): The path to load dataset from
        csv_options (list): a list which may contain one or more of Headers, Scale Data (MinMax)
        label_id (str): The id to assign to the dataset being uploaded.

    Returns:
        tuple: A tuple that contains visibility and content updates for Gradio components.
            (1) Data loading Row - Visibility
            (2) Data Generation Row - Visibility
            (3) data id Textbox - Visibility
            (4) data success Row - Visibility
            (5) success message Markdown - Value
            (6) dataframe State - Value
            (7) Subtitle Markdown - Value
    """
    print(f"Received:\n1)File: {file}\n2)csv options:{csv_options}\n3)Data ID:{data_id}", flush=True)
    if 'Headers' in csv_options:
        headers = 0
    else:
        headers = None
    try:
        df_ = pd.read_csv(file.name, header=headers)
        if 'Scale Data (MinMax)' in csv_options:
            ms = MinMaxScaler()
            df_ = ms.fit_transform(df_)
        gr.Info("Data Uploaded Successfully!")
        return (gr.update(visible=False), gr.update(visible=False), gr.update(visible=False), gr.update(visible=False),
                gr.update(visible=True),
                f"<h1 style='text-align: center; color: #3ebefe;'> Data Loaded Successfully with shape: ({df_.shape[0]},"
                f"{df_.shape[1]}) !</h1>",
                df_,
                data_id,
                f"<h2 style='text-align: center; color: #3ebefe;'>Working on Dataset with ID: {data_id}.</h2>", gr.update(visible=True))

    except Exception as e:
        gr.Info(f"Error in loading data: {e}")
        return str(e), None

def generate_data(synthetic_method, no_instances, no_features, data_id):
    """
    Generates synthetic data based on user's preferences.
    Args:
        synthetic_method (str): Can be one of Blobs or Moons
        no_instances (int): Number of rows to generate
        no_features (int): Number of features to generate
        data_id (str): The assigned dataset label, provided by the user

    Returns:
        tuple: A tuple that contains visibility and content updates for Gradio components.
            (1) Data loading Row - Visibility
            (2) Data Generation Row - Visibility
            (3) data id Textbox - Visibility
            (4) data success Row - Visibility
            (5) success message Markdown - Value
            (6) dataframe State - Value
            (7) Subtitle Markdown - Value
            (8) Title Row - Visibility
    """
    x = None
    if synthetic_method == 'Blobs':
        x, y = make_blobs(n_samples=int(no_instances), n_features=int(no_features))
    elif synthetic_method == 'Moons':
        x, y = make_moons(n_samples=no_instances)
    df_ = pd.DataFrame(x)
    gr.Info("Data Generated Successfully!")
    return (gr.update(visible=False), gr.update(visible=False), gr.update(visible=False),
            gr.update(visible=True),
            f"<h1 style='text-align: center; color: #3ebefe;'> Data Loaded Successfully with shape: ({df_.shape[0]},"
            f"{df_.shape[1]}) !</h1>",
            df_,
            data_id,
            f"<h2 style='text-align: center; color: #3ebefe;'>Working on Dataset with ID: {data_id}.</h2>",
            gr.update(visible=True))

# --------------------(2) MF---------------------------------------------------------

def mf_process(data, operations_status):
    """
    Calculates and saves Meta-Features for a given dataset.
    Args:
        data (gradio.State): The dataset provided or generated by the user

    Returns:
        tuple: A tuple that contains visibility and content updates for Gradio components.
            (1) MF JSON Output Section: Content
            (2) Download MF Button: Visibility
            (3) MF Loaded Title: Content
    """
    # (1) Extract MF
    mfe_ = MFExtractor(data)
    mfe_.calculate_mf()
    gr.Info("Meta Features Extracted Successfully!")

    # (2) Save Results
    mf_results = mfe_.search_mf(search_type="values")
    operations_status["meta-features-extraction"] = True
    try:
        cwd = os.getcwd()
        with open(os.path.join(cwd, "results", "_mf.json"), "w") as f:
            json.dump(mf_results, f)

        return (mf_results, gr.update(value= os.path.join(cwd, "results", "_mf.json"), visible=True),
                operations_status)
    except Exception as e:
        print(e)
        traceback.print_exc()





# --- (3) --- Meta-Learning

def load_model_and_predict(model_choice, data_id):
    # --- (1) load model
    with open(f"repository/meta_learners/models/{model_choice}.pkl", "rb") as f:
        model = pickle.load(f)

    # --- (2) load dataset meta-features/ raise error if they have not been calculated.
    if not os.path.exists("results/_mf.json"):
        raise gr.Error(f"Meta-Feature record for dataset with ID: {data_id} is not present")
    else:
        with open("results/_mf.json", "r") as mf_f:
            mfjson = json.load(mf_f)

    # --- (3) Filter Meta-features to use for prediction
    with open(f"repository/meta_learners/meta-data.json", "r") as mf_f:
        metadata_json = json.load(mf_f)[model_choice]

    mfe = MFExtractor()
    mf_to_keep = []

    if metadata_json["meta-features"]["based_on"] == "Category":
        for sel in metadata_json["meta-features"]["selection"]:
            mf_to_keep += mfe.search_mf(category=sel, search_type="names")
    elif metadata_json["meta-features"]["based_on"] == "Paper":
        for sel in metadata_json["meta-features"]["selection"]:
            mf_to_keep += mfe.search_mf(included_in=sel, search_type="names")


    mfdf = pd.json_normalize({k:v for k,v in mfjson.items() if k in mf_to_keep}).fillna(0)
    prediction = model.predict(mfdf)
    return prediction[0]